{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2b1baa-bcf3-45cc-8485-4029bff49b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# load text files\n",
    "positive_sentences = pd.read_csv(\n",
    "    \"/Users/neilkadian/Downloads/rt-polaritydata/rt-polaritydata/rt-polarity.pos.txt\", header=None, names=[\"text\"], encoding=\"ISO-8859-1\", delimiter=\"\\r\\n\"\n",
    ")\n",
    "negative_sentences = pd.read_csv(\n",
    "    \"/Users/neilkadian/Downloads/rt-polaritydata/rt-polaritydata/rt-polarity.neg.txt\", header=None, names=[\"text\"], encoding=\"ISO-8859-1\", delimiter=\"\\r\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "# add labels: 1 for positive, 0 for negative\n",
    "positive_sentences[\"label\"] = 1\n",
    "negative_sentences[\"label\"] = 0\n",
    "\n",
    "# combine into one DataFrame\n",
    "full_df = pd.concat([positive_sentences, negative_sentences]).reset_index(drop=True)\n",
    "\n",
    "# inspect the dataset\n",
    "print(full_df.head())\n",
    "print(positive_sentences.head())\n",
    "print(negative_sentences.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1216d264-fe50-4529-9440-5a782d014adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define common negation words\n",
    "NEGATION_WORDS = [\"not\", \"never\", \"no\", \"none\", \"n't\", \"cannot\", \"neither\", \"nor\"]\n",
    "\n",
    "def contains_negation(text):\n",
    "    \"\"\"Check if a text contains any negation words.\"\"\"\n",
    "    text = text.lower()\n",
    "    return any(re.search(rf'\\b{neg_word}\\b' if neg_word != \"n't\" else rf\"{neg_word}\\b\", text) for neg_word in NEGATION_WORDS)\n",
    "\n",
    "# add negation column\n",
    "full_df['negation'] = full_df['text'].apply(contains_negation)\n",
    "\n",
    "# split the dataset into train and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    full_df[\"text\"], full_df[\"label\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Size of train_texts: {len(train_texts)}\")\n",
    "print(f\"Size of test_texts: {len(test_texts)}\")\n",
    "\n",
    "# load the DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# tokenize the data\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(list(texts), padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_encodings = tokenize_function(train_texts)\n",
    "test_encodings = tokenize_function(test_texts)\n",
    "\n",
    "# convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": train_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": train_encodings[\"attention_mask\"],\n",
    "    \"labels\": list(train_labels)\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": test_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": test_encodings[\"attention_mask\"],\n",
    "    \"labels\": list(test_labels)\n",
    "})\n",
    "\n",
    "# load DistilBERT model for sequence classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4274780-6918-4da6-ba50-522f5697acb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32b4544-b45b-411e-9327-009e31b85299",
   "metadata": {},
   "source": [
    "### - Visualize negation handling accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aabddaa-be9c-4aa8-adc8-2bedafc6535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define the categories for analysis\n",
    "categories = [\n",
    "    \"Negative Label, Not Negated\",\n",
    "    \"Negative Label, Negated\",\n",
    "    \"Positive Label, Not Negated\",\n",
    "    \"Positive Label, Negated\",\n",
    "]\n",
    "\n",
    "# predict on the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# process test data into input text and true labels\n",
    "test_texts = test_texts.reset_index(drop=True)  # Ensure proper indexing\n",
    "test_labels = test_labels.reset_index(drop=True)\n",
    "\n",
    "# initialize the matrix\n",
    "matrix = np.zeros((4, 2), dtype=int)\n",
    "\n",
    "# fill the matrix\n",
    "for i, text in enumerate(test_texts):\n",
    "    true_label = test_labels[i]\n",
    "    predicted_label = predicted_labels[i]\n",
    "    negation_flag = contains_negation(text)  # use the negation function defined earlier ######\n",
    "\n",
    "    # Determine the row index based on true label and negation status\n",
    "    if true_label == 0:  # negative sentiment\n",
    "        row = 0 if not negation_flag else 1\n",
    "    elif true_label == 1:  # positive sentiment\n",
    "        row = 2 if not negation_flag else 3\n",
    "\n",
    "    # determine the column index based on predicted label\n",
    "    col = predicted_label\n",
    "\n",
    "    # update the matrix\n",
    "    matrix[row, col] += 1\n",
    "\n",
    "# calculate total number of test samples\n",
    "total_test_samples = len(test_texts)\n",
    "\n",
    "# calculate percentages\n",
    "percent_matrix = (matrix / total_test_samples) * 100\n",
    "\n",
    "# visualize the matrix\n",
    "# fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# calculate total negated and non-negated inputs\n",
    "negated_rows = [1, 3]  # Rows corresponding to negated inputs\n",
    "non_negated_rows = [0, 2]  # Rows corresponding to non-negated inputs\n",
    "\n",
    "# correctly labeled inputs for negated and non-negated cases\n",
    "correct_negated = matrix[1, 0] + matrix[3, 1]  # Negated, correct: True Negative + True Positive\n",
    "correct_non_negated = matrix[0, 0] + matrix[2, 1]  # Non-Negated, correct: True Negative + True Positive\n",
    "\n",
    "# total negated and non-negated inputs\n",
    "total_negated = matrix[1, 0] + matrix[1, 1] + matrix[3, 0] + matrix[3, 1]\n",
    "total_non_negated = matrix[0, 0] + matrix[0, 1] + matrix[2, 0] + matrix[2, 1]\n",
    "\n",
    "# calculate overall percentages\n",
    "percent_correct_negated = (correct_negated / total_negated) * 100 if total_negated > 0 else 0\n",
    "percent_correct_non_negated = (correct_non_negated / total_non_negated) * 100 if total_non_negated > 0 else 0\n",
    "\n",
    "# calculate total number of correct predictions\n",
    "correct_predictions = matrix[0, 0] + matrix[1, 0] + matrix[2, 1] + matrix[3, 1]\n",
    "# print(f\" {matrix[0, 0]} \")\n",
    "# print(f\" {matrix[1, 0]} \")\n",
    "# print(f\" {matrix[2, 1]} \")\n",
    "# print(f\" {matrix[3, 1]} \")\n",
    "\n",
    "# calculate overall accuracy\n",
    "overall_accuracy = (correct_predictions / total_test_samples) * 100\n",
    "\n",
    "# visualize the matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "im = ax.imshow(matrix, cmap=\"Blues\", aspect=\"auto\")\n",
    "\n",
    "# annotate the matrix with counts and percentages\n",
    "for i in range(matrix.shape[0]):\n",
    "    for j in range(matrix.shape[1]):\n",
    "        count = matrix[i, j]\n",
    "        percent = percent_matrix[i, j]\n",
    "        ax.text(j, i, f\"{count}\\n({percent:.2f}%)\", ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "# add labels and titles\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks(range(4))\n",
    "ax.set_xticklabels([\"Predicted Negative\", \"Predicted Positive\"])\n",
    "ax.set_yticklabels(categories)\n",
    "ax.set_title(\"Performance Visualization Matrix (Counts and Percentages)\")\n",
    "ax.set_xlabel(\"Predicted Sentiment\")\n",
    "ax.set_ylabel(\"Input Categories\")\n",
    "\n",
    "# add overall percentages below the matrix\n",
    "fig.text(\n",
    "    0.55,\n",
    "    -0.08,\n",
    "    f\"Correctly Labeled Negated Inputs: {percent_correct_negated:.2f}%\\n\"\n",
    "    f\"Correctly Labeled Non-Negated Inputs: {percent_correct_non_negated:.2f}%\\n\"\n",
    "    f\"Overall Accuracy: {overall_accuracy:.2f}%\",\n",
    "    ha=\"center\",\n",
    "    fontsize=12,\n",
    "    color=\"black\",\n",
    ")\n",
    "\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff49c876-197b-47dc-be23-d7e9cfa3ee34",
   "metadata": {},
   "source": [
    "### - Examine the first five examples from each of the 8 categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb982a05-e21f-41c5-b4dd-c64573100c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize matrix and examples dictionary\n",
    "matrix = np.zeros((4, 2), dtype=int)\n",
    "examples = {(row, col): [] for row in range(4) for col in range(2)}\n",
    "\n",
    "# fill the matrix and collect examples\n",
    "for i, text in enumerate(test_texts):\n",
    "    true_label = test_labels[i]\n",
    "    predicted_label = predicted_labels[i]\n",
    "    negation_flag = contains_negation(text)  # Use the negation function defined earlier\n",
    "\n",
    "    # determine the row index based on true label and negation status\n",
    "    if true_label == 0:  # Negative sentiment\n",
    "        row = 0 if not negation_flag else 1\n",
    "    elif true_label == 1:  # Positive sentiment\n",
    "        row = 2 if not negation_flag else 3\n",
    "\n",
    "    # determine the column index based on predicted label\n",
    "    col = predicted_label\n",
    "\n",
    "    # update the matrix\n",
    "    matrix[row, col] += 1\n",
    "\n",
    "    # cllect an example (limit to a small number per category for inspection)\n",
    "    if len(examples[(row, col)]) < 25:\n",
    "        examples[(row, col)].append(text)\n",
    "\n",
    "# print examples for each category\n",
    "categories = [\n",
    "    \"Negative Label, Not Negated\",\n",
    "    \"Negative Label, Negated\",\n",
    "    \"Positive Label, Not Negated\",\n",
    "    \"Positive Label, Negated\",\n",
    "]\n",
    "for row in range(4):\n",
    "    for col in range(2):\n",
    "        print(f\"Category: {categories[row]} | Predicted {'Negative' if col == 0 else 'Positive'}\")\n",
    "        print(f\"Count: {matrix[row, col]}\")\n",
    "        print(\"Examples:\")\n",
    "        for example in examples[(row, col)]:\n",
    "            print(f\"  - {example}\")\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5268c6a8-e8ba-4ab9-8e46-53c083a67a8e",
   "metadata": {},
   "source": [
    "### - Test Specific Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f241e2-e657-46c5-bf30-35cec0290a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# check if MPS is available\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# move the model to the MPS device\n",
    "model = model.to(device)\n",
    "\n",
    "# function to test the model on a single input\n",
    "def test_model_on_input(input_text, model, tokenizer, device):\n",
    "    # tokenize the input\n",
    "    encoding = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "    \n",
    "    # move the tokenized input to the correct device\n",
    "    encoding = {key: value.to(device) for key, value in encoding.items()}\n",
    "    \n",
    "    # pass the tokenized input through the model\n",
    "    with torch.no_grad():  # disable gradient calculation\n",
    "        outputs = model(**encoding)\n",
    "    \n",
    "    # get the predicted label\n",
    "    logits = outputs.logits\n",
    "    predicted_label = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    # map the predicted label to sentiment\n",
    "    sentiment_map = {0: \"Negative\", 1: \"Positive\"}\n",
    "    return sentiment_map[predicted_label]\n",
    "\n",
    "# example usage\n",
    "input_text = \"Made the table no prob but they sent the wrong chairs!? Can't be sad so out enjoying the sunshine\"\n",
    "predicted_sentiment = test_model_on_input(input_text, model, tokenizer, device)\n",
    "print(f\"Input Text: {input_text}\")\n",
    "print(f\"Predicted Sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1133a0-8913-4ed9-9ba4-2be6b2d6332e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
