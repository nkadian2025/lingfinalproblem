{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4d3cff-63e9-4c7e-b396-7db058cc5098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# the CSV should have columns: 'id', 'label', 'tweet'\n",
    "df = pd.read_csv(\"/Users/neilkadian/Downloads/Default Safari Downloads Folder/sentiment_analysis.csv\")\n",
    "\n",
    "# verify the first few rows\n",
    "print(df.head())\n",
    "\n",
    "# define common negation words\n",
    "NEGATION_WORDS = [\"not\", \"never\", \"no\", \"none\", \"n't\", \"cannot\", \"neither\", \"nor\"]\n",
    "\n",
    "def contains_negation(text):\n",
    "    \"\"\"Check if a text contains any negation words.\"\"\"\n",
    "    text = text.lower()\n",
    "    return any(re.search(rf'\\b{neg_word}\\b' if neg_word != \"n't\" else rf\"{neg_word}\\b\", text) for neg_word in NEGATION_WORDS)\n",
    "\n",
    "# define negation and non-negation categories\n",
    "df['negation'] = df['tweet'].apply(contains_negation)\n",
    "\n",
    "# sample the full dataset\n",
    "sample_fraction = 1\n",
    "df_sampled = df.sample(frac=sample_fraction, random_state=42)\n",
    "\n",
    "# split the dataset into train and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df_sampled[\"tweet\"], df_sampled[\"label\"], test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Size of train_texts: {len(train_texts)}\")\n",
    "print(f\"Size of test_texts: {len(test_texts)}\")\n",
    "\n",
    "# Load the DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# tokenize the data\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(list(texts), padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_encodings = tokenize_function(train_texts)\n",
    "test_encodings = tokenize_function(test_texts)\n",
    "\n",
    "# convert to Hugging Face Dataset format\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": train_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": train_encodings[\"attention_mask\"],\n",
    "    \"labels\": list(train_labels)\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": test_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": test_encodings[\"attention_mask\"],\n",
    "    \"labels\": list(test_labels)\n",
    "})\n",
    "\n",
    "# load DistilBERT model for sequence classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb7bfa4-b3a1-489f-b439-923dc4190367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "# fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32b4544-b45b-411e-9327-009e31b85299",
   "metadata": {},
   "source": [
    "### - Visualize negation handling accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aabddaa-be9c-4aa8-adc8-2bedafc6535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define the categories for analysis\n",
    "categories = [\n",
    "    \"Negative Label, Not Negated\",\n",
    "    \"Negative Label, Negated\",\n",
    "    \"Positive Label, Not Negated\",\n",
    "    \"Positive Label, Negated\",\n",
    "]\n",
    "\n",
    "# Predict on the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# process test data into input text and true labels\n",
    "test_texts = test_texts.reset_index(drop=True)  # Ensure proper indexing\n",
    "test_labels = test_labels.reset_index(drop=True)\n",
    "\n",
    "# initialize the matrix\n",
    "matrix = np.zeros((4, 2), dtype=int)\n",
    "\n",
    "# fill the matrix\n",
    "for i, text in enumerate(test_texts):\n",
    "    true_label = test_labels[i]\n",
    "    predicted_label = predicted_labels[i]\n",
    "    negation_flag = contains_negation(text)  # Use the negation function defined earlier ######\n",
    "\n",
    "    # determine the row index based on true label and negation status\n",
    "    if true_label == 1:  # Negative sentiment\n",
    "        row = 0 if not negation_flag else 1\n",
    "    elif true_label == 0:  # Positive sentiment\n",
    "        row = 2 if not negation_flag else 3\n",
    "\n",
    "    # determine the column index based on predicted label\n",
    "    col = 0 if predicted_label == 1 else 1\n",
    "\n",
    "    # Update the matrix\n",
    "    matrix[row, col] += 1\n",
    "\n",
    "# calculate total number of test samples\n",
    "total_test_samples = len(test_texts)\n",
    "\n",
    "# calculate percentages\n",
    "percent_matrix = (matrix / total_test_samples) * 100\n",
    "\n",
    "# visualize the matrix\n",
    "# fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Calculate total negated and non-negated inputs\n",
    "negated_rows = [1, 3]  # rows corresponding to negated inputs\n",
    "non_negated_rows = [0, 2]  # rows corresponding to non-negated inputs\n",
    "\n",
    "# correctly labeled inputs for negated and non-negated cases\n",
    "correct_negated = matrix[1, 0] + matrix[3, 1]  # negated, correct: True Negative + True Positive\n",
    "correct_non_negated = matrix[0, 0] + matrix[2, 1]  # Non-Negated, correct: True Negative + True Positive\n",
    "\n",
    "# Total negated and non-negated inputs\n",
    "total_negated = matrix[1, 0] + matrix[1, 1] + matrix[3, 0] + matrix[3, 1]\n",
    "total_non_negated = matrix[0, 0] + matrix[0, 1] + matrix[2, 0] + matrix[2, 1]\n",
    "\n",
    "# calculate overall percentages\n",
    "percent_correct_negated = (correct_negated / total_negated) * 100 if total_negated > 0 else 0\n",
    "percent_correct_non_negated = (correct_non_negated / total_non_negated) * 100 if total_non_negated > 0 else 0\n",
    "\n",
    "# Calculate total number of correct predictions\n",
    "correct_predictions = matrix[0, 0] + matrix[1, 0] + matrix[2, 1] + matrix[3, 1]\n",
    "# print(f\" {matrix[0, 0]} \")\n",
    "# print(f\" {matrix[1, 0]} \")\n",
    "# print(f\" {matrix[2, 1]} \")\n",
    "# print(f\" {matrix[3, 1]} \")\n",
    "\n",
    "# ccalculate overall accuracy\n",
    "overall_accuracy = (correct_predictions / total_test_samples) * 100\n",
    "\n",
    "# visualize the matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "im = ax.imshow(matrix, cmap=\"Blues\", aspect=\"auto\")\n",
    "\n",
    "# annotate the matrix with counts and percentages\n",
    "for i in range(matrix.shape[0]):\n",
    "    for j in range(matrix.shape[1]):\n",
    "        count = matrix[i, j]\n",
    "        percent = percent_matrix[i, j]\n",
    "        ax.text(j, i, f\"{count}\\n({percent:.2f}%)\", ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "# add labels and titles\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks(range(4))\n",
    "ax.set_xticklabels([\"Predicted Negative\", \"Predicted Positive\"])\n",
    "ax.set_yticklabels(categories)\n",
    "ax.set_title(\"Performance Visualization Matrix (Counts and Percentages)\")\n",
    "ax.set_xlabel(\"Predicted Sentiment\")\n",
    "ax.set_ylabel(\"Input Categories\")\n",
    "\n",
    "# Add overall percentages below the matrix\n",
    "fig.text(\n",
    "    0.55,\n",
    "    -0.08,\n",
    "    f\"Correctly Labeled Negated Inputs: {percent_correct_negated:.2f}%\\n\"\n",
    "    f\"Correctly Labeled Non-Negated Inputs: {percent_correct_non_negated:.2f}%\\n\"\n",
    "    f\"Overall Accuracy: {overall_accuracy:.2f}%\",\n",
    "    ha=\"center\",\n",
    "    fontsize=12,\n",
    "    color=\"black\",\n",
    ")\n",
    "\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff49c876-197b-47dc-be23-d7e9cfa3ee34",
   "metadata": {},
   "source": [
    "### - Examine the first few examples from each of the 8 categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb982a05-e21f-41c5-b4dd-c64573100c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize matrix and examples dictionary\n",
    "matrix = np.zeros((4, 2), dtype=int)\n",
    "examples = {(row, col): [] for row in range(4) for col in range(2)}\n",
    "\n",
    "# Fill the matrix and collect examples\n",
    "for i, text in enumerate(test_texts):\n",
    "    true_label = test_labels[i]\n",
    "    predicted_label = predicted_labels[i]\n",
    "    negation_flag = contains_negation(text)  # Use the negation function defined earlier\n",
    "\n",
    "    # determine the row index based on true label and negation status\n",
    "    if true_label == 1:  # Negative sentiment\n",
    "        row = 0 if not negation_flag else 1\n",
    "    elif true_label == 0:  # Positive sentiment\n",
    "        row = 2 if not negation_flag else 3\n",
    "\n",
    "    # deetermine the column index based on predicted label\n",
    "    col = predicted_label\n",
    "\n",
    "    # Update the matrix\n",
    "    matrix[row, col] += 1\n",
    "\n",
    "    # collect an example (limit to a small number per category for inspection)\n",
    "    if len(examples[(row, col)]) < 78:\n",
    "        examples[(row, col)].append(text)\n",
    "\n",
    "# print examples for each category\n",
    "categories = [\n",
    "    \"Negative Label, Not Negated\",\n",
    "    \"Negative Label, Negated\",\n",
    "    \"Positive Label, Not Negated\",\n",
    "    \"Positive Label, Negated\",\n",
    "]\n",
    "for row in range(4):\n",
    "    for col in range(2):\n",
    "        print(f\"Category: {categories[row]} | Predicted {'Positive' if col == 0 else 'Negative'}\")\n",
    "        print(f\"Count: {matrix[row, col]}\")\n",
    "        print(\"Examples:\")\n",
    "        for example in examples[(row, col)]:\n",
    "            print(f\"  - {example}\")\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5268c6a8-e8ba-4ab9-8e46-53c083a67a8e",
   "metadata": {},
   "source": [
    "### - Test Specific Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f241e2-e657-46c5-bf30-35cec0290a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# check if MPS is available\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# move the model to the MPS device\n",
    "model = model.to(device)\n",
    "\n",
    "# function to test the model on a single input\n",
    "def test_model_on_input(input_text, model, tokenizer, device):\n",
    "    # tokenize the input\n",
    "    encoding = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "    \n",
    "    # Move the tokenized input to the correct device\n",
    "    encoding = {key: value.to(device) for key, value in encoding.items()}\n",
    "    \n",
    "    # pass the tokenized input through the model\n",
    "    with torch.no_grad():  # disable gradient calculation\n",
    "        outputs = model(**encoding)\n",
    "    \n",
    "    # Get the predicted label\n",
    "    logits = outputs.logits\n",
    "    predicted_label = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    # map the predicted label to sentiment\n",
    "    sentiment_map = {1: \"Negative\", 0: \"Positive\"}\n",
    "    return sentiment_map[predicted_label]\n",
    "\n",
    "# example usage\n",
    "input_text = \"Just switched from iphone to android. I'm in love and don't regret it! #samsung #GlobeGalaxyS4LTE\"\n",
    "predicted_sentiment = test_model_on_input(input_text, model, tokenizer, device)\n",
    "print(f\"Input Text: {input_text}\")\n",
    "print(f\"Predicted Sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b4ea67-6740-4cce-a777-dc356f809b50",
   "metadata": {},
   "source": [
    "### Make Saliency graph for specific input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1133a0-8913-4ed9-9ba4-2be6b2d6332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency(input_text, model, tokenizer, device):\n",
    "    # tokenize input text\n",
    "    encoding = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # get embeddings from input_ids\n",
    "    embeddings = model.get_input_embeddings()(input_ids)\n",
    "    embeddings.requires_grad_()  # enable gradient computation for embeddings\n",
    "    embeddings.retain_grad()  # retain gradients for the embeddings\n",
    "\n",
    "    # forward pass\n",
    "    outputs = model(inputs_embeds=embeddings, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # compute gradients with respect to the predicted class\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    loss = logits[0, predicted_class]  # Focus on the predicted class\n",
    "    loss.backward()\n",
    "\n",
    "    # compute token-level saliency scores (gradient magnitude)\n",
    "    gradients = embeddings.grad.abs().sum(dim=-1).squeeze().cpu().numpy()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # remove `[PAD]` tokens\n",
    "    valid_indices = attention_mask[0].cpu().numpy().astype(bool)\n",
    "    tokens = [tokens[i] for i in range(len(tokens)) if valid_indices[i]]\n",
    "    gradients = gradients[valid_indices]\n",
    "\n",
    "    return tokens, gradients\n",
    "\n",
    "# example usage\n",
    "input_text = \"I have to give some to #apple for the #Iphone . I dropped my #iphone in the sink today and not one problem. Thank you #Apple !\"\n",
    "tokens, gradients = compute_saliency(input_text, model, tokenizer, device)\n",
    "\n",
    "# plot saliency\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(tokens, gradients, color=\"skyblue\")\n",
    "plt.xlabel(\"Saliency Score\")\n",
    "plt.title(\"Gradient-Based Interpretability\")\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for readability\n",
    "plt.show()\n",
    "\n",
    "predicted_sentiment = test_model_on_input(input_text, model, tokenizer, device)\n",
    "print(f\"Input Text: {input_text}\")\n",
    "print(f\"Predicted Sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507cb3bd-d3c5-47ea-9b5d-c0babfe415ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
